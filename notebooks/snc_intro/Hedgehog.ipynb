{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hedgehog\n",
    "*(macros here; code in appendices)* \n",
    "$%Discrete Model$\n",
    "$\\newcommand{\\nbuff}[]{n_b} $\n",
    "$\\newcommand{\\nact}[]{n_a} $\n",
    "$\\newcommand{\\nres}[]{n_r} $\n",
    "$\\newcommand{\\state}[1][t]{\\mathbf{x}(#1)} $\n",
    "$\\newcommand{\\action}[1][t]{\\mathbf{u}(#1)} $\n",
    "$\\newcommand{\\arrival}[1][t]{\\mathbf{a}(#1)} $\n",
    "$\\newcommand{\\constituency}[]{\\mathbf{C}} $\n",
    "$\\newcommand{\\processing}[1][t]{\\mathbf{B}(#1)} $\n",
    "$\\newcommand{\\cost}[]{\\mathbf{c}} $\n",
    "$\\newcommand{\\costfn}[]{f_{\\cost}} $\n",
    "$%Fluid Model$\n",
    "$\\newcommand{\\flstate}[1][t]{\\mathbf{q}(#1)} $\n",
    "$\\newcommand{\\flactivitycumul}[1][t]{\\mathbf{z}(#1)} $\n",
    "$\\newcommand{\\flactivitycumulfree}[]{\\mathbf{z}} $\n",
    "$\\newcommand{\\flactivity}[1][t]{\\mathbf{\\zeta}(#1)} $\n",
    "$\\newcommand{\\flarrival}[]{\\mathbf{\\alpha}} $\n",
    "$\\newcommand{\\flprocessing}[]{\\mathbf{B}} $\n",
    "$%Workload$\n",
    "$\\newcommand{\\nwork}[]{n_w} $\n",
    "$\\newcommand{\\workload}[]{\\mathbf{\\xi}} $\n",
    "$\\newcommand{\\workloadmat}[]{\\mathbf{\\Xi}} $\n",
    "$\\newcommand{\\workloadmathat}[]{\\hat{\\mathbf{\\Xi}}} $\n",
    "$\\newcommand{\\bottleneck}[]{\\mathbf{\\nu}} $\n",
    "$\\newcommand{\\worktime}[1][]{\\mathbf{w}#1} $\n",
    "$\\newcommand{\\loadbalance}[]{\\mathbf{\\rho}} $\n",
    "$\\newcommand{\\effcost}[]{\\mathbf{\\bar{c}}} $\n",
    "$\\newcommand{\\effcostfn}[]{f_{\\effcost}} $\n",
    "$\\newcommand{\\monotone}[]{\\mathbb{W}^+ } $\n",
    "$\\newcommand{\\idledirection}[]{\\mathbf{v}^*}$\n",
    "$\\newcommand{\\idlingset}[]{\\mathbb{K}}$\n",
    "$\\newcommand{\\nonidlingset}[]{\\mathbb{S}}$\n",
    "$%Hedging$\n",
    "$$\\newcommand{\\hedging}[]{\\beta^*}$$\n",
    "$%SafetyStocks$\n",
    "$$\\newcommand{\\safetystockpenalty}[]{\\mathbf{\\psi}}$$\n",
    "$%Generic$\n",
    "$\\newcommand{\\policy}[]{\\phi} $\n",
    "$\\newcommand{\\T}[]{\\top} $\n",
    "$%Derived Symbols$\n",
    "$\\newcommand{\\statetzero}[]{\\mathbf{x}(0)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 . Whats the Problem?\n",
    "\n",
    "A network is composed of three elemets: *buffers*, *actions* and *resources*.\n",
    "* A *buffer* is a queue, whose state is the number of items in it.\n",
    "* An *action* changes the state of certain buffers.\n",
    "* A *resource* is an agent that can perform certain actions, one per time step.\n",
    "\n",
    "The topology and dynamics of the network is specified by two matrices and a vector:\n",
    "* The *constituency matrix* indicates which resources can perform which actions. No two resources can perform the same action.\n",
    "* The *buffer processing matrix* indicates what the effect of certain actions will have at time $t$ on the buffer states\n",
    "* The *arrivals vector* indicates what new items enter each buffer at time $t$, from outside the system.\n",
    "\n",
    "The evolution of the states $\\state$ over time is a simple Markov Decision Process, that is also a Controlled Random Walk (CRW) - a random walk, where one element of stochasticity can be controlled by actions $\\action$. \n",
    "\n",
    "$$ \\state[t+1] = \\state + \\processing[t+1]\\action + \\arrival[t+1]$$ \n",
    "\n",
    "where $\\processing$ is the processing matrix and $\\arrival$ is the arrivals vector.\n",
    "\n",
    "Our policy $\\policy$, which is a mapping from states to actions, is the controlled part of the random walk: $\\policy : \\mathbb{X} \\rightarrow \\mathbb{U}$.\n",
    "\n",
    "$$ \\state[t+1] = \\state + \\processing[t+1]\\policy(\\state) + \\arrival[t+1]$$ \n",
    "\n",
    "#### Objective\n",
    "\n",
    "We define a cost at each time step, which is a function of the state of all buffers. In our case this is a linear function (i.e. each buffer has a fixed cost per item in queue).\n",
    "\n",
    "$$\\text{step cost} = \\costfn(\\state) = \\cost^\\T\\state$$\n",
    "\n",
    "We then define a value function $v^\\policy(\\mathbf{x}')$, which is the expected sum of discounted future cost under a policy function $\\policy$, given the current state. By minimising this functional, we will find our optimal behaviour - the $\\policy^*$ function.\n",
    "\n",
    "$$\\policy^* = \\text{argmin}_{\\policy} v^\\policy(\\mathbf{x}') = \\text{argmin}_{\\policy} \\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^tf_c(\\state) |  \\state[0] = \\mathbf{x'}]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How to solve it?\n",
    "\n",
    "The solution to the CRW functional is intractable. We solve with a number of approximations, and find a solution that is asymptotically optimal under heavy-load and heavy-traffic assumptions.\n",
    "\n",
    "* **heavy load** - new items arrival rate is close to the slowest processing rate. \n",
    "* **heavy traffic** - there are always enough items in the buffer to do all actions.\n",
    "\n",
    "The algorithm to solve our problem is outlined below...\n",
    "\n",
    "### 2.0. Spoilers\n",
    "**TL;DR:** *Our approximately optimal policy (under heavy-load, heavy-traffic assumptions) is simple: critical resources (bottlenecks) can never idle, except in the special case where them idling helps us strategically lower the cost in the long run. Our online algorithm finds these non-idling bottlenecks, then runs an optimisation to find activities by greedily minimising the cost. A penalty term is added to this optimisation so no buffers are _completely_ empty, so all activities are possible ('safety stocks'). Our final activities can be translated into actions probabilistically over a time period.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Find the set of non-idling bottleneck ($\\nonidlingset$)**\n",
    "    1. **We transform our problem into a new 'workload' space.** From a buffer-state space, we move to a space where each dimension corresponds the amount of time each resource needs to work to drain the network. The idea is that we can approximate our problem by only looking at the resources which take the longest to drain the network - the bottlenecks. This reduces the dimensionality and provides with a good physical intuition with how policies move us in the space: \n",
    "        * *Idling a single resource, moves you up a coordinate axis (+ve).*\n",
    "        * *Working a single resource, moves you down a coordinate axis (-ve).*\n",
    "    2. **We find the nearest point in the 'monotone region'.** In our workload space, we can plot our effective cost - it looks like a piecewise linear function made of intersecting hyperplanes. In this space, there is a region where moving towards the origin along any axis is guaranteed to lower the effective cost. This is the monotone region.\n",
    "        * *Inside the monotone region our optimal policy is pretty obvious -  work all resources all the time!*\n",
    "        * *Outside this region, a step to the origin may actually increase the cost - lets idle towards the monotone region!*\n",
    "    3. **We calculate a hedging distance, from the 'monotone region'.** The asymptotically optimal policy in terms of total cost in a _non-stochastic_ environment, is  to move to the monotone region by idling certain resources, and once you're there, work all resources all the time. In fact, to compensate for stochastic fluctuations, we actually only need to idle until we get to a fixed distance from the monotone region, before we work all out. This fixed distance is called the hedging distance and it differs depending on which face of the region you are approaching. Our policy now becomes: \n",
    "        * *Inside the ~~monotone region~~ hedging region -  work all resources all the time!*\n",
    "        * *Outside this region - lets idle towards the hedging region*!\n",
    "4. **Calculate the safety stock parameter ($\\safetystockpenalty$)** Our algorithm relies on an assumption that resources can always work when they are told to - i.e. the buffers are never fully empty. However, this is not guaranteed by our algorithm. We calculate a penalty to add to the cost when our resources approach starvation to compensate for this.\n",
    "5. **Choose activity rates (activities) to minimize the step cost, given non-idling bottleneck constraints and safety stock penalty$\\safetystockpenalty$** This is simply the result of the constrained activation, where we greedily solve to minimise the step cost given our non-dling constraints and safety stock penalty vector.\n",
    "6. **Translate these continuous activity rates to discrete actions over a given time period** Each activity (which is a real number) is converted to a discrete series of actions over a time period, probabilistically.\n",
    "\n",
    "Now we move to the model in greater detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 The Fluid Model\n",
    "\n",
    "Our fundamental model is discrete, which makes it hard to solve. We start by approximating our CRW by a fluid model. This can be considered approximation of the network on very long timescales - \"zooming out\" on the time axis. The implications of this are:\n",
    "* our time is now continuous\n",
    "* our state is now continuous: $\\state \\rightarrow \\flstate$\n",
    "* our arrivals can be replaced by an expected arrival rate: $\\arrival \\rightarrow \\flarrival$\n",
    "* our processing matrix can be replaced by an expected processing matrix: $\\processing \\rightarrow \\flprocessing$\n",
    "* our actions are now an action rate or 'activity': $\\action \\rightarrow \\flactivity$\n",
    "* $\\therefore$ our system is now deterministic\n",
    "\n",
    "\n",
    "Note that to define the instantaneous activity, we can start by defining the 'cumulative activity' $\\flactivitycumul$, that represents how much each action has been worked on by time $t$. By taking the derivative of this with respect to time, we get our instantaneous activity  $\\flactivity = \\dfrac{d\\flactivitycumul}{dt}$ . It is assumed $\\flactivity \\in \\mathbb{U} $ for all $ t$.\n",
    "\n",
    "Our instantaneous dynamics are now:\n",
    "\n",
    "$$ \\dfrac{d\\flstate[t]}{dt} =  \\flprocessing\\flactivity + \\flarrival$$ \n",
    "\n",
    "Or, in terms of cumulative change:\n",
    "\n",
    "$$ \\flstate = \\flstate[0] + \\flprocessing\\flactivitycumul + \\flarrival t$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Workload Space\n",
    "\n",
    "Even with these continuous dynamics, the solution is still intractable. Our first move is to transform the problem into a new space, in which the problem is expressed in terms of \"bottlenecks\". \n",
    "\n",
    "These bottlenecks are the resources of the network that most greatly affect the draining time of the network, and are critical to the dynamics of the system. Our state space transformation will move us from buffer space (where each axis marks the number of items in a specific buffer) to workload space (where each axis marks the amount of time a specific resource must work in order to drain their buffers).\n",
    "\n",
    "By only choosing the most loaded resources to be axes in this space, we approximate our problem by reducing its dimensionality. This is analagous to finding the k-leading singular values in a large matrix problem, and solving problems in this reduced subspace. Note that, since we often have more buffers than resources, workload space is already a reduced dimensionality representation, but the key approximation to our system comes from not considering resources which are not the bottlenecks of our problem. \n",
    "\n",
    "Note that the total activities that each bottleneck should perform can be collected into a vector, called the worktime vectors $\\worktime[(t)]$. As such, we seek a transformation from $\\state \\rightarrow \\worktime[(t)]$, which is linear $\\worktime[(t)] = \\workloadmat \\state$. But how do we find which resources are bottlenecks? And how do we find the transformation $\\workloadmat$  to go from buffer state to worktime per resource? \n",
    "\n",
    "We consider a thought experiment: \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thought Experiment\n",
    "\n",
    "*Imagine the network starting at time $t=0$ with a state of $\\mathbf{q}_1$.  How would we choose activities for all the resources to minimise the amount of time to drain the network from this state to a lower state $\\mathbf{q}_2$, assuming **no new arrivals**?*\n",
    "\n",
    "This is a constrained optimisation problem. We seek to find the minimal $T$, subject to a number of physical constraints:\n",
    "\n",
    "1. The change in fluid of the system occurs over time $T$. In terms of the cumulative change dynamics equation:\n",
    "$$ \\mathbf{q}_2 = \\mathbf{q}_1 + \\flprocessing\\flactivitycumul[T] + 0$$ \n",
    "\n",
    "2. Secondly each resource $r$ can not have worked more than $T$ time in total, across all it's activities. Since no two resources work on the same action, each row of $\\constituency$ multiplied by the cumulative activity, $\\flactivitycumul$, picks out the sum of activities that the resource has done. This must be less than $T$. As a matrix equation:\n",
    "\n",
    "$$\\constituency\\flactivitycumul[T] \\leq \\mathbf{1}T =  \\mathbf{T}$$\n",
    "\n",
    "\n",
    "3. Finally it is not physical to have a negative activity, nor indeed negative time.\n",
    "$$ \\flactivitycumulfree \\geq \\mathbf{0}$$\n",
    "$$T \\geq 0$$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thought Experiment: Our Optimisation\n",
    "\n",
    "Our constrained optimisation is thus:\n",
    "$$\n",
    " \\begin{array}{ccccccc|ccc}\n",
    "\\text{Primal} &&&&  \\text{Dual}  &&&& \\textit{Dual Rewritten}\\\\\n",
    "\\hline\n",
    "\\min_{\\mathbf{z}, T} T     &&&& \\max_{\\workload, \\bottleneck} \\workload^\\T( \\mathbf{q}_1 -  \\mathbf{q}_2) &&&& \\max_{\\workload, \\bottleneck} \\workload^T\\flarrival \\\\\n",
    "\\text{ s.t.}               &&&& \\text{ s.t.}  &&&& \\text{ s.t.} \\\\\n",
    "\\mathbf{q}_2 = \\mathbf{q}_1 + \\flprocessing \\flactivitycumulfree &&&&   \\flprocessing^\\T \\workload + \\constituency^\\T\\bottleneck \\geq \\mathbf{0}  &&&& - \\flprocessing^T \\workload - \\constituency^T\\bottleneck \\leq \\mathbf{0}\\\\\n",
    "\\constituency \\flactivitycumulfree \\leq \\mathbf{T} &&&& - \\mathbf{1}^\\T\\bottleneck + 1 \\geq 0 &&&& \\mathbf{1}^T\\bottleneck \\leq 1 \\\\\n",
    "\\flactivitycumulfree \\geq \\mathbf{0} &&&& \\bottleneck \\geq \\mathbf{0} &&&&  -\\mathbf{I} \\bottleneck \\leq \\mathbf{0}\\\\\n",
    "T \\geq 0 &&&&\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Note that in the last case, we have let  $\\mathbf{q}_1 = \\flarrival$, $\\mathbf{q}_2 = 0$ - i.e we drain to empty from an initial state of $\\flarrival$. We can thus solve this succinct linear optimisation to find our $\\workload$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Workload Matrix and Relaxed Workload Matrix\n",
    "Of course the solution to the optimisation will only return the workload vector $\\workload$ corresponding to the slowest resource - the biggest bottleneck.  We actually want the vectors corresponding to many more resources!\n",
    "\n",
    "One way of finding these is simply to eliminate that resource for the problem and recalculate, though in practice it is simpler to note that that constraints can be written as a block matrix of constraints of form $\\mathbf{Ax} \\leq \\mathbf{b}$:\n",
    "\n",
    "\n",
    "$$\\Big[ \\begin{array}{c c}\n",
    "-\\flprocessing^\\T & -\\constituency^\\T \\\\\n",
    "\\hline\n",
    "\\mathbf{0} & -\\mathbf{I} \\\\\n",
    "\\hline\n",
    "vec(0)^\\T & vec(1)^\\T \\\\\n",
    "\\end{array} \\Big]\n",
    "\\Big[  \\begin{array}{c}\n",
    "\\workload \\\\\n",
    "\\hline\n",
    "\\bottleneck \\\\\n",
    "\\end{array}\\Big]\n",
    "\\leq\n",
    "\\Big[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{0} \\\\\n",
    "\\hline\n",
    "\\mathbf{0}  \\\\\n",
    "\\hline\n",
    "1 \\\\\n",
    "\\end{array} \\Big]\n",
    "$$\n",
    "\n",
    "These constraints define a polyhedron in workload space, and given our maximisation objective $\\workload^\\T\\flarrival$ is linear - all maximisations in this space lie along the vertices of this polyhedron. We therefore find the vertices of this polyhedron using a vertex enumeration library, each giving us a bottleneck $\\bottleneck$ and workload vector $\\workload$.\n",
    "\n",
    "Each $\\bottleneck$ indicates to us which resource in particular the workload vector refers to, since it is a vector that lies on the simplex over resources. It will have non-zero value on the resource to which it's $\\workload$ refers.\n",
    "\n",
    "We can also collect our workload vectors $\\workload$ into rows of a matrix $\\workloadmat = [\\workload_1, ..., \\workload_{\\nres}]^\\T$, ordered decreasingly by their the time it takes for them to drain an arrival at a single time step - \"the load balancing fraction\": $\\loadbalance_i = \\workload_i^\\T \\flarrival$. This $\\loadbalance_i$ is necessarily fraction, because if it is greater than one then clearly the buffer state will grow unboundedly at that resource and the network can never be drained. Note that our \"heavy load\" approximation can now be well defined: $\\max_i \\loadbalance_i \\approx 1$\n",
    "\n",
    "\n",
    "Now, having a workload matrix $\\workloadmat = [\\workload_1, ..., \\workload_{\\nres}]^\\T$ with rows sorted largest to smallest by $\\loadbalance$, our approximation is to keep only the resources that appear to be bottlenecks - those with load balancing fraction greater than some constant: say $\\loadbalance_i > 0.7$. This gives our new \"relaxed\" workload matrix:\n",
    "\n",
    "$$\\workloadmathat =  \\workloadmat_{[:\\nwork, :]} = [\\workload_1, ..., \\workload_{\\nwork}]^\\T;  \\nwork \\leq \\nres $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Workload Policies\n",
    "\n",
    "#### The Problem in Workload Space\n",
    "\n",
    "Our problem has been transformed into the workload space ($\\workloadmat$) and relaxed ($\\workloadmathat$), such that we are now addressing the problem in terms of $ \\worktime = \\workloadmathat \\state$. This worktime $\\worktime$ is the time that each of our chosen bottlenecks needs to work to drain their \"fluid buffers\".  In this space, we can define our problem with a new \"effective cost\" function $\\effcostfn$, defined in relation to our original cost. \n",
    "\n",
    "$$\\effcostfn(\\worktime) = \\min_x \\cost^\\T \\state\\\\ \\text{ s.t. } \\\\\\workloadmathat \\state = \\worktime $$\n",
    "\n",
    "We take a minimum over $\\state$ because there may be multiple different $\\state$ states that have the same worktime $\\worktime = \\workloadmathat \\state$, and we define our effective cost as the minimum of these possible states, since this minimum will correspond to the cost associated with the buffer states affected by our bottlenecks (those buffers which arent affected by our bottlenecks).\n",
    "\n",
    "This minimisations can once again be expressed in terms of a dual optimisation, which is a simple linear function subject to some inequality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    " \\begin{array}{ccccccc}\n",
    "\\text{Primal}  &&&&  \\text{Dual}  \\\\\n",
    "\\hline\n",
    "\\min_{\\state} \\cost^\\T\\state            &&&& \\max_{\\effcost} \\effcost^\\T \\worktime \\\\\n",
    "\\text{ s.t.}                       &&&& \\text{ s.t.} \\\\\n",
    "\\workloadmathat \\state = \\worktime &&&&  \\workloadmathat^\\T \\effcost \\leq \\cost \\\\\n",
    "\\state \\geq 0                      &&&&  \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the inequality constraints define a polyhedron and the solution to which $\\effcost$ maximises the cost is the vertex of this polyhedron, picked out by the slope of the linear function (in this case our $\\worktime$). We could find all these $k$ different possible solutions $\\{\\effcost_k\\}$ by vertex enumeration if we wished, or we could simply perform the above optimisation to find the exact value of the effective cost function $\\effcostfn$ at our current $\\worktime$\n",
    "\n",
    "Eithey way our effective cost function $\\effcostfn$ in $\\worktime$-space thus clearly a piecewise linear function, made up as a maximum over k overlapping hyperplanes, defined by vectors $\\{\\effcost^k\\}$.\n",
    "\n",
    "$$\\effcostfn(\\worktime) = \\max_k {\\effcost^k}^\\T \\worktime$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Example Effective Cost Surface \n",
    "%matplotlib notebook\n",
    "cbar1 = np.array([-2, 2])\n",
    "cbar2 = np.array([1, 1.5])\n",
    "cbar3 = np.array([3,-3])\n",
    "# cbar4 = np.array([0,-5])\n",
    "cbars = [cbar1, cbar2, cbar3]\n",
    "\n",
    "f_cbar = lambda w : np.max(np.array(cbars) @ w, axis=0)\n",
    "\n",
    "plot_piecewise_workspace2D(cbars, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Policies in Workspace\n",
    "We have now:\n",
    "* an approximation to map our buffer state process $\\state$ into a (lower-dimensional) worktime state process $\\worktime [(t)]= \\workloadmathat \\state$\n",
    "* an approximation to the cost function in this space $\\effcostfn(\\worktime) = \\max_{k} {\\effcost^k}^\\T \\worktime$\n",
    "\n",
    "In this new workload space, the coordinate axes have a clear interpretable meaning - each axis represents the time a particular resource must work to drain its buffers (assuming no new arrivals). Navigating the workspace can now easily be done by  choosing actions for particular resources:\n",
    "* Moving along up a coordinate axis (increasing our worktime for one resource) corresponds to idling that particular resource as arrivals continue.\n",
    "* Moving down an axis to the origin implies servicing a buffer fast than arrivals appear. \n",
    "\n",
    "However we still have to find our optimal policy to move about in this workload space. \n",
    "\n",
    "*Q: How do we define the policy what to idle and not idle, to optimise the cost?*\n",
    "\n",
    "A: In this new space, the components of $\\effcost^k$ are particularly important, as they indicate the change in effective cost upon moving along a coordinate axis in worktime. In a region where all components are positive, our optimal policy is well defined - step towards the origin $\\implies$ non-idle all bottlenecks. This region is knows as the monotone region:\n",
    "\n",
    "$$\\monotone = \\{\\worktime : \\effcostfn(\\worktime') \\geq \\effcostfn(\\worktime) \\text{ for all } \\worktime' \\geq \\worktime,\\worktime' \\in \\mathbb{W}  \\}$$\n",
    "\n",
    "Outside of this region, it is not clear whether a random step of idling on not idling will necessarily lower the cost - especially in high dimensions. Our optimal policy is to get to the monotone region as quickly as possible, before going flat-out on not-idling. This can be done by idling a single bottleneck (which moves us along a coordinate axis) into the monotone region.  Our aim will be to find the closest face of the monotone region, and allow our final optimisation to idle in that direction, while constraining all other bottlenecks to non-idle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([5, 0.3])\n",
    "load = np.array([0.7, 0.9])\n",
    "\n",
    "\n",
    "origin = np.zeros_like(w)\n",
    "main_path = get_full_paths(w, cbars, load)\n",
    "_, w_star, l = main_path[0] # note: w_star only first \n",
    "    \n",
    "paths = [\n",
    "    [(w, origin , drift_vec(-w, load))],\n",
    "    [(w, w_star, l), (w_star, origin, drift_vec(-w_star, load))],\n",
    "    main_path\n",
    "]\n",
    "\n",
    "annotations = get_w_star_annotations(w, w_star)\n",
    "annotations += get_path_annotations(paths)\n",
    "annotations += get_load_annotations(w, load)\n",
    "plot_piecewise_workspace2D(cbars, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_paths(cbars, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Nearest Face to the Monotone Region\n",
    "\n",
    "As it stands, we know we occupy a point in workspace, and want to get to the montone region but we have no analytic idea where the monotone region exists. However we know that if the monotone region is accessible by idling, the point of minimum cost for all points that are reachable by idling *must be in the monotone region*. \n",
    "\n",
    "$$\\worktime^* = \\text{argmin}_{\\worktime'} \\effcost^\\T\\worktime' \\text{ s.t. } \\worktime' \\geq \\worktime,  \\implies \\worktime^* \\in \\monotone$$\n",
    "\n",
    "\n",
    "----\n",
    "*Quick Argument*\n",
    "1. Assume the $\\worktime^*$ that minimises $\\effcostfn = \\effcost^\\T\\worktime $ is not in the monotone region.\n",
    "2. Since the cost function is convex along each coordinate direction/axis, if we consider a step along any coordinate axis:\n",
    "    * If any step lowers the cost, the point is not the minimum.\n",
    "    * If no steps lower the cost - the point must be in the monotone region.\n",
    "3. Proof by contradiction, 1. must be False. \n",
    "----\n",
    "Knowing this proof, we simply find $\\worktime^*$ by performing this exact minimisation, solving with the above LP (or its dual). \n",
    "\n",
    "Our result is $\\worktime^*$ gives us a direction $\\idledirection$ in which we may idle, and the components of these vector which are non-zero are those resources which may idle. All other components of this vector (which are zero) correspond to resources which must non-idle and be constrained to do so. In this way we define two sets of bottlenecks: the idling set $\\idlingset$ and the non-idling set $\\nonidlingset$.\n",
    "\n",
    "$$\\idledirection := \\dfrac{ \\worktime^* - \\worktime}{| \\worktime^* -\\worktime|} $$\n",
    "$$\\idlingset := \\{i: \\idledirection_i > 0\\}$$\n",
    "$$\\nonidlingset := \\{i: \\idledirection_i = 0\\}$$\n",
    "$$\\text{where}$$\n",
    "$$\\idlingset \\cap \\nonidlingset = \\mathbb{0} \\text{ and } |\\idlingset| + |\\nonidlingset| = \\nwork $$\n",
    "\n",
    "We have now found our non-idling set  $\\nonidlingset$, which we will be constrained to work, when we solve for our optimal actions, back in buffer space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Hedging\n",
    "\n",
    "We have now translated the random walk of our buffer state process $\\state$ into an approximate lower-dimensional worktime process $\\worktime[(t)]$.\n",
    "This worktime process is also controlled random walk, where our controlled decision is whether to work or idle each bottleneck resource. We have seen that our optimal policy in this workload space is to nonidle all resources in the monotone region $\\monotone$, and outside $\\monotone$, to idle $\\idlingset$ towards it in the $\\idledirection$ direction. In buffer space, this policy corresponds to constraining all nonidling $\\nonidlingset$ to non-idle, while solving a greedily minimising the cost. (This should still take us towards the monotone region).\n",
    "\n",
    "However, we have neglected a large part the model so far - the stochasticity of all the processes. Given our arrivals and servicings are stochastic, any decisions to work or idle will be subject to noise, bumping us randomly through the workspace. Given the stochasticity of the process, it may not be worth idling all the way until we are in the monotone region before we set all resources to work - instead we might just need to idle to fixed distance from the nearest face before we go 'all out' on our nonidling.\n",
    "\n",
    "Indeed this is what Sean has proved in his book - that the asymptotically optimal policy is simply an affine translation (ie a parallel shift) of each of the faces of the monotone region. The distance that each face $i$ must be shifted is called the hedging distance $\\hedging_i$. Since the number of faces of $\\monotone$ grows combinatorially with dimension of the space, we calculate each hedging distance online. This means we only refer to the one hedging distance we are interested in at a time, and can drop our index $i$: $\\hedging_i \\rightarrow \\hedging$.  \n",
    "\n",
    "#### Hedging Policy\n",
    "Our policy now becomes:\n",
    "* If we are in $\\monotone$ - constrain all bottlenecks to non-idle\n",
    "* If we are within $\\hedging$ of the nearest face of $\\monotone$ - constrain all bottlenecks to non-idle\n",
    "* Else, act as before - allow some bottlenecks to idle, to move us towards the monotone/enlarged region\n",
    "\n",
    "An example image is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_lim = [-2, 20]\n",
    "y_lim = [-2, 20]\n",
    "\n",
    "monotone_point = [4,4]\n",
    "\n",
    "# find gradients of intersection w monotone region -> define affine hedging regions\n",
    "axis_beta_stars = [.7, -1.5]\n",
    "grads = get_grads_of_intersection(cbars, monotone_point) \n",
    "annotations = get_hedging_region_annotations(grads, axis_beta_stars, x_lim, y_lim)\n",
    "\n",
    "noise = .25\n",
    "drift = -(1 - load)\n",
    "grad, beta_star = grads[-1], axis_beta_stars[-1] # lower surface gradient and beta star\n",
    "drift_g = gradient_to_drift_scale(grad, drift) # make a drift vector out of gradient \n",
    "\n",
    "start = [15,  15 * grad] # start on intersection\n",
    "seconds = 40\n",
    "noise_per_second = 5\n",
    "\n",
    "noisy_paths = generate_noisy_paths(start, drift_g, grad, noise, seconds, noise_per_second)\n",
    "\n",
    "path_annotations = get_noisy_path_annotations(grad, drift_g, [beta_star, 0], noisy_paths)\n",
    "\n",
    "annotations += path_annotations\n",
    "annotations += get_load_annotations([0,0], load)\n",
    "\n",
    "plot_piecewise_workspace2D(cbars, annotations, contour_only=True, x_lim=x_lim, y_lim=y_lim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hedging Intuition\n",
    "\n",
    "It can be difficult to intuit why stochasticity should increase a hedging distance. To illustrate the need for hedging, first consider starting at $\\worktime^*$, and non_idling continuously towards the origin. For the sake of simplicity, we assume the direction in which we are non idling - the drift - is the same as the gradient.\n",
    "\n",
    "In a deterministic environment, this paths is a straight line. In a stochastic environment, where spherical noise is drawn and added to the state at each step, we obtain a random walk. However, under our policy, whenever we move below the gradient slope, our y_component idles - resulting in a _reflected random walk_ along the gradient axis.\n",
    "\n",
    "Samples of possible paths are shown below, with the noiseless walk, random walk, and reflected random walk plotted together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3_example_walks(cbars, load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that our random walk is reflected along a the intersecting line of two hyperplanes ( called it the 'reflection axis') clearly affects the distribution of possible states that the random walk can be in. Previously, the mean of the random walk would have been the drift - however now that the mean of the walk is shifted, away from the reflection  axis. This change (from random walk to reflected random walk) is visible below, using a sample of 1000 random walks, with the quartiles and means plotted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "grad = grads[-1] # lower surface gradient and beta star\n",
    "drift = gradient_to_drift_scale(grad, drift) # make a drift vector out of gradient \n",
    "# drift = - (1 - load)\n",
    "\n",
    "plot_distributions_walks(cbars, drift, 0.25, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above it is clear that to minimise the total cost, we will need to shift the new trajectory of states vertically so the more states now lie along the valley defined by reflection axis. However, it should be noted, not all states are weighed equally - cost is discounted with time! This brings us to our final right most plot: this is the distribution of all states encountered across all sample paths, with each step weighted by its discount factor. This is the 'cloud' that we want to shift to minimise our future cost.\n",
    "\n",
    "With this in mind we can intuit the factors that impact our shift (our hedging):\n",
    "* The relative steepness of the two faces in question along the axis parallel to the reflection axis:\n",
    "    * If monotone/non-monotone is high, hedging is high\n",
    "    * If monotone/non-monotone is low, hedging is low\n",
    "* The drift direction of the random walk:\n",
    "    * If drift is away from the axis -> More hedging\n",
    "    * If drift is towards than the axis -> Less Hedging \n",
    "* The noise of the random walk:\n",
    "    * A noisier walk -> A bigger step -> More hedging\n",
    "* The discount factor:\n",
    "    * A discount factor of 0, suggests you should start at the minimum -> No hedging\n",
    "    * A discount factor of 1, suggests you should factor in the very long term effects of drift -> High Hedging\n",
    "\n",
    "These are the 4 factors that impact hedging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hedging In Practice\n",
    "\n",
    "#### Hedging Assumptions\n",
    "\n",
    "\n",
    "It is worth noting the assumptions that lead to our hedging calculation. First of all, it is clear from our graphs that we assume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Safety Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing is to consider that, under our current policy, whene\n",
    "Consider a guided random walk along the cost surface, starting at the edge of the monotone region, in which we consistently guided towards the origin. \n",
    "\n",
    "\n",
    "\n",
    "* solve linear program to get to nearest face.\n",
    "* but we dont want to move exactly to the nearest face, because the random walk will push us around. its a waste. at an optimum point we just start moving to origin, because the random walk will push us around the steep surface.\n",
    "    * can we have a negative beta? ie move - into the monotone region because the final surface is too steep\n",
    "    * idling corresponds to moving along a coordinate axis, but doesnt the rest move (non-idle?) so actual policy is not that straight line to the axis?\n",
    "* we calculate that distance with a hedging parameter based on the cost profile and the statistics of the random walk\n",
    "* this distance (an affine enlargement of the monotone region) indicates our policy: if outside region -> idle towards it. otherwise, move to origin by non-idling on everything!\n",
    "\n",
    "### Safety Stocks\n",
    "* An approximation thats made is that, when non-idling, work can always be done on the buffers. \n",
    "* in a fluid model thats not a problem, but with discrete packets, cant do work on an empty buffer\n",
    "* we add constraints to indicate which buffers cannot be empty, and find a penalty function on emptying buffers. \n",
    "* This will return a vector which will be added to our cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback Policy\n",
    "We have obtained \n",
    "\n",
    "Since we have obtained the our final cost, constraints on which resources can and cant idle (ie which direction we need to move in workload space), we finally perform a minimization to find our optimum 'activities' over a fixed time horizon.\n",
    "\n",
    "Once we have these optimum activities we then convert these activities into a probabilistic policy, and return decisions for the next t times steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.A. Glossary\n",
    "### Discrete Model\n",
    "\n",
    "| Name | Desc |  Tex | Symbol | \n",
    "|:-----|:-----:| ---- | ------ |\n",
    "|**Number of Buffers**| A scalar indicating the number of buffers in the system. | `\\nbuff` | $$\\nbuff \\in \\mathbb{Z_+}$$|\n",
    "|**Number of Activities**| A scalar indicating the number of activities (or possible actions) in the system. | `\\nact` | $$\\nact \\in \\mathbb{Z_+}$$|\n",
    "|**Number of Resources**| A scalar indicating the number of resources (or workers) in the system. | `\\nres` | $$\\nres \\in \\mathbb{Z_+}$$|\n",
    "|**State**| A vector indicating the number of items in each buff in the system. | `\\state` | $$\\state \\in \\mathbb{Z}_+^{\\nbuff}$$|\n",
    "|**Arrival**| A vector indicating the arrivals arriving at each buffer at time step $t$ | `\\arrival` | $$\\arrival \\in \\mathbb{Z}_+^{\\nbuff}$$|\n",
    "|**Action**| A vector indicating all the actions taken by the whole system at time step $t$ | `\\action` | $$\\action \\in \\{0,1\\}^{\\nact}$$|\n",
    "|**Constituency Matrix**| A binary matrix indicating the which actions each resource can perform simultaneously. <br><br> *eg: row $r$ indicates which actions resource $r$ can perform simultaneously*  | `\\constituency` | $$\\constituency \\in \\{0,1\\}^{\\nres \\times \\nact}$$|\n",
    "|**Processing Matrix**| A matrix indicating the possible changes in buffer state for each action at time $t$. <br><br>  *eg: column $c$ indicates the changes in each buffer size at time $t$ due to action $c$*  | `\\processing` | $$\\processing \\in \\mathbb{Z}^{\\nbuff \\times \\nact}$$|\n",
    "|**Cost**| A vector indicating the cost associated with each buffer.| `\\cost` | $$\\cost : \\mathbb{R}^{\\nbuff}$$|\n",
    "|**Cost Function**| A function mapping buffer state to a real value. This is usually linear | `\\costfn` | $$\\costfn : \\mathbb{X}^{\\nbuff} \\to \\mathbb{R} $$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluid Model\n",
    "| Name | Desc |  Tex | Symbol | \n",
    "|:-----|:-----:| ---- | ------ |\n",
    "|**Fluid State**| A continuous version of the state. | `\\flstate` | $$\\flstate \\in \\mathbb{R}_+^{\\nbuff}$$|\n",
    "|**Fluid Cumulative Activity**| A vector indicating the cumulative activity (time) each action has been worked on at time $t$. | `\\flactivitycumul` | $$\\flactivitycumul \\in \\mathbb{R}_+^{\\nact}$$|\n",
    "|**Fluid Activity Rate**| The instantaneous change in fluid cumulative activity with respect to time | `\\flactivity` | $$\\flactivity  := \\dfrac{d\\flactivitycumul}{dt}$$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workload Space\n",
    "| Name | Desc |  Tex | Symbol | \n",
    "|:-----|:-----:| ---- | ------ |\n",
    "|**Workload Vector**| A vector corresponding to a resource, indicating how much it must work on each buffer to maximally drain the fluid | `\\workload` | $$\\workload \\in \\mathbb{R}^{\\nbuff}$$|\n",
    "|**Workload Matrix**| A matrix where each row corresponds to how much a resource must work on each buffer to maximally drain the fluid | `\\workloadmat` | $$\\workloadmat = [\\workload_1^\\T, ..., \\workload_{\\nres}^\\T ], \\workloadmat\\in \\mathbb{R}^{\\nres \\times \\nbuff }$$|\n",
    "|**Bottleneck**| An indicator vector signalling a resource, which corresponds to a particular workload vector| `\\bottleneck` | $$\\bottleneck \\in \\mathbb{R}_+^{\\nres}$$|\n",
    "|**Number of Workload Vectors**| A scalar indicating the number of workload vectors in our relaxation. | `\\nwork` | $$\\nwork < \\nres, \\nwork \\in \\mathbb{Z_+}$$|\n",
    "|**Workload Matrix Approximator**|  A approximator matrix to $\\workloadmat$,  keeping only the leading $\\nwork$ workload $\\workload$ vectors | `\\workloadmathat` | $$\\workloadmathat  = [\\workload_1^\\T, ..., \\workload_{\\nwork}^\\T ], \\workloadmathat \\in \\mathbb{R}^{\\nwork \\times \\nbuff}$$|\n",
    "|**Worktime Process**| A vector corresponding to time each buffer must be worked on (maximally) to drain the buffer to empty (assuming no arrivals)| `\\worktime` | $$\\worktime := \\workloadmathat \\state, \\worktime \\in \\mathbb{R}_+^{\\nwork}$$|\n",
    "|**Load-Balance (Time) Vector**| A vector corresponding to time ('activity') each buffer must be worked on (maximally) to drain just arrivals| `\\loadbalance` | $$\\loadbalance := \\workloadmathat \\flarrival, \\loadbalance \\in \\mathbb{R}_+^{\\nwork}$$ |\n",
    "|**Effective Cost**| A vector indicating the effective cost associated with the worktime process (of the bottlenecks).| `\\effcost` | $$\\effcost : \\mathbb{R}^{\\nbuff}$$|\n",
    "|**Effective Cost Function**| A function mapping worktime process to a real value. This is piecewise-linear. | `\\effcostfn` | $$\\effcostfn : \\mathbb{W}^{\\nwork} \\to \\mathbb{R} $$|\n",
    "|**Monotone Region**| The region of workspace where all components of effective cost level sets are non-negative. A step along any axis towards origin does not increase the effective cost. | `\\monotone` | $$\\monotone$$|\n",
    "|**Idle Direction**| The unit direction pointing to the nearest point the monotone region that is reachable by idling | `\\idledirection` | $$\\idledirection$$|\n",
    "|**Idling Set**| The set of indices referring to the bottlenecks that do not need to be constrained to non-idle/work flat out| `\\idlingset` | $$\\idlingset$$|\n",
    "|**Non-Idling Set**| The set of indices referring to the bottlenecks that need to be constrained to non-idle/work flat out| `\\nonidlingset` | $$\\nonidlingset$$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedging Space\n",
    "| Name | Desc |  Tex | Symbol | \n",
    "|:-----|:-----:| ---- | ------ |\n",
    "|**Hedging Distance**| A scalar indicating the distance to nearest surface of the monotone region, which defines a hedging region | `\\hedging` | $$\\hedging \\in \\mathbb{R}$$|\n",
    "|**Workload Matrix**| A matrix where each row corresponds to how much a resource must work on each buffer to maximally drain the fluid | `\\workloadmat` | $$\\workloadmat = [\\workload_1^\\T, ..., \\workload_{\\nres}^\\T ], \\workloadmat\\in \\mathbb{R}^{\\nres \\times \\nbuff }$$|\n",
    "|**Bottleneck**| An indicator vector signalling a resource, which corresponds to a particular workload vector| `\\bottleneck` | $$\\bottleneck \\in \\mathbb{R}_+^{\\nres}$$|\n",
    "|**Number of Workload Vectors**| A scalar indicating the number of workload vectors in our relaxation. | `\\nwork` | $$\\nwork \\leq \\nres, \\nwork \\in \\mathbb{Z_+}$$|\n",
    "|**Workload Matrix Approximator**|  A approximator matrix to $\\workloadmat$,  keeping only the leading $\\nwork$ workload $\\workload$ vectors | `\\workloadmathat` | $$\\workloadmathat  = [\\workload_1^\\T, ..., \\workload_{\\nwork}^\\T ], \\workloadmathat \\in \\mathbb{R}^{\\nwork \\times \\nbuff}$$|\n",
    "|**Worktime Process**| A vector corresponding to time each buffer must be worked on (maximally) to drain the buffer to empty (assuming no arrivals)| `\\worktime` | $$\\worktime := \\workloadmathat \\state, \\worktime \\in \\mathbb{R}_+^{\\nwork}$$|\n",
    "|**Load-Balance (Time) Vector**| A vector corresponding to time ('activity') each buffer must be worked on (maximally) to drain just arrivals| `\\loadbalance` | $$\\loadbalance := \\workloadmathat \\flarrival, \\loadbalance \\in \\mathbb{R}_+^{\\nwork}$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.A.1 Effective Cost Plotting Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SURFACE PATH CODE\n",
    "import numpy as np\n",
    "\n",
    "def get_cbar(c_bars, w):\n",
    "    f_cbar = np.array(c_bars)\n",
    "    return c_bars[np.argmax(f_cbar @ w)]\n",
    "\n",
    "def find_intersection(cbars, this_cbar, w, drift):\n",
    "    f_cbar = np.array(cbars)\n",
    "    diff = np.ma.array(f_cbar - this_cbar)\n",
    "    lambdas = - diff @ w / (diff @ drift)\n",
    "    valid = lambdas[np.where((lambdas > 0 ))[0]]\n",
    "    \n",
    "    return sorted(valid)[0] + 1e-10\n",
    "        \n",
    "def get_crossing_point(cbars, w, load):\n",
    "    this_c_bar = get_cbar(cbars, w)\n",
    "    is_monotone =  np.all(this_c_bar>=0)\n",
    "    if is_monotone:\n",
    "        drift = drift_vec(np.ones_like(w)*-1, load)\n",
    "    else:\n",
    "        drift = (this_c_bar < 0).astype(int) * load\n",
    "    l = find_intersection(cbars, this_c_bar, w, drift)\n",
    "    w_dot = w + l * drift \n",
    "    return w_dot, drift, is_monotone\n",
    "\n",
    "def get_full_paths(w, cbars, load):\n",
    "    s = w\n",
    "    points = []\n",
    "    is_monotone = np.all(get_cbar(cbars, w)>=0)\n",
    "    while np.any(s > 0.0001) and not is_monotone:   \n",
    "        e, d, is_monotone = get_crossing_point(cbars, s, load)\n",
    "        points.append((s,e,d))\n",
    "        s = e\n",
    "    points.append((s,np.zeros(w.shape),d))\n",
    "    return points\n",
    "\n",
    "def drift_vec(v, load):\n",
    "    direc = np.vstack([load, -(1-load)])\n",
    "    return direc[(v < 0).astype(np.int64), np.arange(len(load))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTTING FUNCTIONS \n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DPI = 250\n",
    "def plot_piecewise_workspace2D(cbars, points, dpi=250, contour_only=False, x_lim=None, y_lim=None):\n",
    "    x_lim = [-2, 6] if x_lim is None else x_lim\n",
    "    y_lim = [-2, 6] if y_lim is None else y_lim\n",
    "    \n",
    "    def f(x, y):\n",
    "        f_cbar = np.array(cbars)\n",
    "        v = np.array([x,y])\n",
    "        return np.max(np.einsum( 'ij,jkl->ikl', f_cbar, v), axis = 0)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    ax = fig.add_subplot(1,2,1,projection='3d')\n",
    "\n",
    "    xvalues = np.linspace(x_lim[0], x_lim[1], dpi)\n",
    "    yvalues = np.linspace(y_lim[0], y_lim[1], dpi)\n",
    "\n",
    "    xgrid, ygrid = np.meshgrid(xvalues, yvalues)\n",
    "    zvalues = f(xgrid, ygrid)\n",
    "\n",
    "    surf = ax.plot_surface(xgrid, ygrid, zvalues, rstride=5, cstride=5,linewidth=0, cmap=cm.plasma)\n",
    "    \n",
    "    if not contour_only:\n",
    "        for x, y, style in points:\n",
    "            plt.plot(x,y,f([x],[y])[0], style)\n",
    "    ax.set_xlim(*x_lim)\n",
    "    ax.set_ylim(*y_lim)\n",
    "    ax.view_init(elev=30., azim=230)\n",
    "       \n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.set_xlim(*x_lim)\n",
    "    ax.set_ylim(*y_lim)\n",
    "    plt.contourf(xgrid, ygrid, zvalues, 30,cmap=cm.plasma)\n",
    "    fig.colorbar(surf, aspect=18)\n",
    "    plt.plot([0, 0], y_lim, '-.k', lw=0.3)\n",
    "    plt.plot(x_lim, [0, 0], '-.k', lw=0.3)\n",
    "    \n",
    "    for x, y, style in points:\n",
    "        plt.plot(x,y, style,lw=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "      \n",
    "def get_w_star_annotations(w, w_star, colour='k'):\n",
    "    w_plot = [[i] for i in w] + ['x'+colour]\n",
    "    w_star_plot = [[i] for i in w_star] + ['*' + colour]\n",
    "    return [w_plot, w_star_plot]    \n",
    "\n",
    "def get_path_annotations(paths, colour='k'):\n",
    "    annotations = []\n",
    "    fmt = [':', '-.', '-', '--']\n",
    "    for i, path in enumerate(paths):\n",
    "        for s, e, _ in path:\n",
    "            annotations.append([[si,ei] for si, ei in zip(s,e)] + [fmt[i] + colour])\n",
    "    return annotations\n",
    "\n",
    "def get_load_annotations(w, load):\n",
    "    nonidling_direc = w - np.diag(1 - load)  \n",
    "    idling_direc = w + np.diag(load) \n",
    "    annotations = []\n",
    "    for ii, n in enumerate(nonidling_direc):\n",
    "        arrow = [i for i in zip(n)] + ['+w']\n",
    "        v = [[i,j] for i,j in zip(w, n)] + ['-w']\n",
    "        annotations.extend([arrow, v])\n",
    "    for jj, n in enumerate(idling_direc):\n",
    "        arrow = [i for i in zip(n)] + ['+w']\n",
    "        v = [[i,j] for i,j in zip(w, n)] + ['-w']\n",
    "        annotations.extend([arrow, v])\n",
    "    return annotations\n",
    "\n",
    "def get_cost_for_line(cbars, start, end, drift):\n",
    "    def f(x, y):\n",
    "        f_cbar = np.array(cbars)\n",
    "        v = np.array([x,y])\n",
    "        return np.max(np.einsum( 'ij,jkl->ikl', f_cbar, v), axis = 0)\n",
    "    dpi = int(np.abs(np.floor(100*(np.linalg.norm(end-start)/drift))))\n",
    "    \n",
    "    line = []\n",
    "    for s, e in zip(start,end):\n",
    "        line.append([np.linspace(s, e, dpi)])\n",
    "    \n",
    "    return f(*line)[0]\n",
    "\n",
    "def line_drift_speed(start, end, load):\n",
    "    return load @ (end-start)/np.linalg.norm(end-start)\n",
    "\n",
    "def to_cumul(path, gamma=1):\n",
    "    t1 = len(path)\n",
    "    discount = [gamma**i for i in range(t1)]\n",
    "    cumul = np.tril(np.ones((t1,t1)) * discount) @ path\n",
    "    return cumul\n",
    "\n",
    "def plot_paths(cbars, paths):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    gamma = 0.95\n",
    "    \n",
    "    x_max = 1000\n",
    "    \n",
    "    for lines in paths:\n",
    "        path_points = []\n",
    "        for start, end, load in lines:\n",
    "            d = line_drift_speed(start, end, load)\n",
    "            points = get_cost_for_line(cbars, start, end, d)\n",
    "            \n",
    "            path_points.append(points)\n",
    "        path = np.hstack(path_points)\n",
    "        ax1.plot(np.arange(len(path))/100, path)\n",
    "        cumul = to_cumul(path, gamma)\n",
    "        ax2.plot(np.arange(len(cumul[:x_max]))/100, cumul[:x_max],)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.A.2 Hedging Plotting Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HEDGING PATH CODE\n",
    "import math \n",
    "\n",
    "def get_grads_of_intersection(cbars, monotone_point):\n",
    "    monotone_cbar = get_cbar(cbars, monotone_point)\n",
    "    neg = np.array([1,-1])\n",
    "    diff =  (np.array(cbars) * neg) - (monotone_cbar * neg)\n",
    "    diff = diff[~np.all(diff == 0, axis=1)]\n",
    "    grads =  diff[:, 0 ] / diff[:, 1]\n",
    "    return grads\n",
    "\n",
    "def generate_noisy_changes(start, drift, grad, noise_scale, secs, dots_per_sec):\n",
    "    dps = dots_per_sec\n",
    "    points = secs * dps\n",
    "    drift = drift / dps    \n",
    "    \n",
    "    noise = np.random.normal(scale=noise_scale, size=(2, points)) \n",
    "    policy = np.ones_like(noise) * drift[:, None] \n",
    "    coord_tot = np.array(start) \n",
    "    \n",
    "    walk_change = (noise + policy)\n",
    "    walk_change[:, 0] = np.array(start) \n",
    "    return walk_change\n",
    "\n",
    "def reflect_noisy_changes(walk_change):\n",
    "    ct = np.zeros(2)\n",
    "    for i, c in enumerate(walk_change.T):\n",
    "        ct += c\n",
    "        if ct[1] - ct[0] * grad < 0:\n",
    "            walk_change[1, i] += - (ct[1] - ct[0] * grad)\n",
    "            ct[1] +=  -(ct[1] - ct[0] * grad)\n",
    "    return walk_change\n",
    "\n",
    "def generate_noisy_paths(start, drift, grad, noise_scale, secs, dots_per_sec):\n",
    "    walk_change = generate_noisy_changes(start, drift, grad, noise_scale, secs, dots_per_sec)\n",
    "    walk_change = reflect_noisy_changes(walk_change)\n",
    "    return np.array([to_cumul(walk_change[0]), to_cumul(walk_change[1])])\n",
    "\n",
    "\n",
    "def generate_illustrative_rw_paths(start, drift, grad, noise_scale, secs, dots_per_sec):\n",
    "    noiseless_change = generate_noisy_changes(start, drift, grad, 1e-10, secs, dots_per_sec)\n",
    "    noiseless =  np.array([to_cumul(noiseless_change[0]), to_cumul(noiseless_change[1])])\n",
    "    walk_change = generate_noisy_changes(start, drift, grad, noise_scale, secs, dots_per_sec)\n",
    "    unreflected =  np.array([to_cumul(walk_change[0]), to_cumul(walk_change[1])])\n",
    "    walk_change2 = reflect_noisy_changes(walk_change)\n",
    "    reflected =  np.array([to_cumul(walk_change2[0]), to_cumul(walk_change2[1])])\n",
    "    return noiseless,  unreflected, reflected\n",
    "\n",
    "def generate_many_illustrative_paths(samples, start, drift, grad, noise_scale, secs, dots_per_sec):\n",
    "    rrw_p = []\n",
    "    rw_p = []\n",
    "    for i in range(samples):\n",
    "        _, unref, ref = generate_illustrative_rw_paths(start, drift, grad, noise_scale, secs, dots_per_sec)\n",
    "        rw_p.append(unref)\n",
    "        rrw_p.append(ref)\n",
    "    return np.array(rw_p), np.array(rrw_p)\n",
    "\n",
    "def gradient_to_drift_scale(grad, drift):\n",
    "    return - np.array([1,grad]) * np.linalg.norm(drift)/ np.linalg.norm([1, grad])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING FUNCTIONS\n",
    "\n",
    "def get_hedging_region_annotations(grads, beta_star, x_lim=None, y_lim=None):\n",
    "    annotation = []\n",
    "    x_lim = np.array([-2, 6]) if x_lim is None else np.array(x_lim)\n",
    "    y_lim = np.array([-2, 6]) if y_lim is None else np.array(y_lim)\n",
    "    for g, b in zip(grads, beta_star):\n",
    "        \n",
    "        cos = 1 / ((1 + g*g)**.5)\n",
    "        sin = g / ((1 + g*g)**.5)\n",
    "        \n",
    "        annotation.append([x_lim, g*x_lim + b/cos, '--w'])\n",
    "        annotation.append([x_lim, g*(x_lim), ':w'])\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def plot_cumul_cost(f_cbar, paths, gamma):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    x_max = 1000\n",
    "    \n",
    "    for lines in paths:\n",
    "        path = f_cbar(np.vstack(lines))\n",
    "        ax1.plot(np.arange(len(path))/100, path)\n",
    "        cumul = to_cumul(path, gamma)\n",
    "        ax2.plot(np.arange(len(cumul[:x_max]))/100, cumul[:x_max],)\n",
    "    ax1.legend(-0.1 * np.arange(len(paths)))\n",
    "    ax2.legend(-0.1 * np.arange(len(paths)))\n",
    "\n",
    "def get_noisy_path_annotations(grad, drift, beta_stars, noisy_path, fmts=['-', '--', '-.']):\n",
    "    annotation = []\n",
    "\n",
    "    cos = 1/ ((1 + grad*grad)**.5)  \n",
    "    \n",
    "    for fmt, beta_star in zip(fmts,beta_stars):\n",
    "        x_path_hedge = noisy_path[0]\n",
    "        y_path_hedge = noisy_path[1] + beta_star/cos\n",
    "        annotation.append([x_path_hedge, y_path_hedge, fmt + 'k'])\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3_example_walks(cbars, load):\n",
    "    \n",
    "    x_lim = [0, 10]\n",
    "    y_lim = [0, 10]\n",
    "\n",
    "    # find gradients of intersection w monotone region -> define affine hedging regions\n",
    "    monotone_point = [4,4]\n",
    "    grads = get_grads_of_intersection(cbars, monotone_point) \n",
    "    annotations = [get_hedging_region_annotations(grads, [0,0], x_lim, y_lim)  for _ in range(3)] \n",
    "\n",
    "    noise = 0.25\n",
    "    drift = - (1 - load)\n",
    "    grad = grads[-1] # lower surface gradient and beta star\n",
    "    drift_g = gradient_to_drift_scale(grad, drift) # make a drift vector out of gradient \n",
    "\n",
    "    start = [8,  8 * grad] # start on intersection\n",
    "    seconds = 20\n",
    "    noise_per_second = 3\n",
    "    for a in annotations:\n",
    "        illustrative_paths = generate_illustrative_rw_paths(start, drift_g, grad, noise, seconds, noise_per_second)\n",
    "        path_annotations = []\n",
    "        for ip, fmt in zip(illustrative_paths, ['-', '--', '-']):\n",
    "            path_annotations += get_noisy_path_annotations(grad, drift_g, [0], ip, [fmt])\n",
    "        a += path_annotations\n",
    "        a += get_load_annotations([0,0], load)\n",
    "\n",
    "    gamma = 0.95\n",
    "\n",
    "    plot_piecewise_contours(cbars, annotations, gamma, x_lim=x_lim, y_lim=y_lim)\n",
    "    \n",
    "def plot_piecewise_contours(cbars, paths, gamma, dpi=250, x_lim=None, y_lim=None):\n",
    "    x_lim = [-2, 6] if x_lim is None else x_lim\n",
    "    y_lim = [-2, 6] if y_lim is None else y_lim\n",
    "    \n",
    "    def f(x, y):\n",
    "        f_cbar = np.array(cbars)\n",
    "        v = np.array([x,y])\n",
    "        return np.max(np.einsum( 'ij,jkl->ikl', f_cbar, v), axis = 0)\n",
    "\n",
    "    cols = len(paths)\n",
    "    rows = 2\n",
    "    fig = plt.figure(figsize=( 9.5, rows * 9.5/cols))\n",
    "\n",
    "\n",
    "    xvalues = np.linspace(x_lim[0], x_lim[1], dpi)\n",
    "    yvalues = np.linspace(y_lim[0], y_lim[1], dpi)\n",
    "\n",
    "    xgrid, ygrid = np.meshgrid(xvalues, yvalues)\n",
    "    zvalues = f(xgrid, ygrid)\n",
    "    \n",
    "    plots = len(paths)\n",
    "    for i, p in enumerate(paths):\n",
    "        ax1 = fig.add_subplot(rows,cols,i+1)\n",
    "\n",
    "        surf = plt.contourf(xgrid, ygrid, zvalues, 30, cmap=cm.plasma)\n",
    "        plt.plot([0, 0], y_lim, '-.k', lw=0.3)\n",
    "        plt.plot(x_lim, [0, 0], '-.k', lw=0.3)\n",
    "        for x, y, style in p:\n",
    "            plt.plot(x,y, style,lw=0.7)\n",
    "\n",
    "        ax1.set_xlim(*x_lim)\n",
    "        ax1.set_ylim(*y_lim)\n",
    "        fig.colorbar(surf, aspect=18, orientation='horizontal')\n",
    "\n",
    "    for i, p in enumerate(paths):\n",
    "        ax2 = fig.add_subplot(rows,cols,i+1 + cols)\n",
    "        for x, y, style in p[4:7]:\n",
    "            path = f_cbar(np.vstack([x,y]))\n",
    "            cumul = to_cumul(path, gamma=gamma)\n",
    "            ax2.plot(np.arange(len(cumul)), cumul,)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribs(cbars, dist, annotations, all_paths, gamma, dpi=250, x_lim=None, y_lim=None):\n",
    "    x_lim = [-2, 6] if x_lim is None else x_lim\n",
    "    y_lim = [-2, 6] if y_lim is None else y_lim\n",
    "    \n",
    "    def f(x, y):\n",
    "        f_cbar = np.array(cbars)\n",
    "        v = np.array([x,y])\n",
    "        return np.max(np.einsum( 'ij,jkl->ikl', f_cbar, v), axis = 0)\n",
    "\n",
    "    cols = 3\n",
    "    rows = 1\n",
    "    fig = plt.figure(figsize=( 9.5, rows * 9.5/cols))\n",
    "\n",
    "    xvalues = np.linspace(x_lim[0], x_lim[1], dpi)\n",
    "    yvalues = np.linspace(y_lim[0], y_lim[1], dpi)\n",
    "\n",
    "    xgrid, ygrid = np.meshgrid(xvalues, yvalues)\n",
    "    zvalues = f(xgrid, ygrid)\n",
    "    \n",
    "    plots = len(paths)\n",
    "    for i in range(2):\n",
    "        ax1 = fig.add_subplot(rows,cols,i+1)\n",
    "        surf = plt.contourf(xgrid, ygrid, zvalues, 30, cmap=cm.plasma)\n",
    "        \n",
    "        plt.plot([0, 0], y_lim, '-.k', lw=0.3)\n",
    "        plt.plot(x_lim, [0, 0], '-.k', lw=0.3)\n",
    "        for x, y, style in annotations:\n",
    "            plt.plot(x,y, style,lw=0.7)\n",
    "            \n",
    "        plt.plot(*dist[i][0],  '-k', lw=0.7)\n",
    "        plt.plot(*dist[i][1],  '--k', lw=0.7)\n",
    "        plt.plot(*dist[i][2],  '--k', lw=0.7)\n",
    "        ax1.set_xlim(*x_lim)\n",
    "        ax1.set_ylim(*y_lim)\n",
    "        fig.colorbar(surf, aspect=18, orientation='horizontal')\n",
    "        \n",
    "    ax3 = fig.add_subplot(rows,cols,3)\n",
    "    bins = 50\n",
    "    Z = np.zeros((bins+1,bins+1))\n",
    "    x1 = np.arange(bins + 2) * (x_lim[1] - x_lim[0])/bins\n",
    "    y1 = np.arange(bins + 2) * (y_lim[1] - y_lim[0])/bins\n",
    "    plt.plot([0, 0], y_lim, '-.k', lw=0.3)\n",
    "    plt.plot(x_lim, [0, 0], '-.k', lw=0.3)\n",
    "    for x, y, style in annotations:\n",
    "        plt.plot(x,y, style,lw=0.7)\n",
    "    \n",
    "    for i in range(all_paths.shape[2]):\n",
    "        H, xb, yb = np.histogram2d(all_paths[:, 0, i], all_paths[:, 1, i], bins=np.array([x1, y1]))\n",
    "        Z += H * (gamma ** i)\n",
    "    Z /= (all_paths.shape[0] * all_paths.shape[2])\n",
    "\n",
    "    xm, ym = np.meshgrid(xb[:-1], yb[:-1])\n",
    "    surf = plt.contourf(xgrid, ygrid, zvalues, 30, alpha=0.5, cmap=cm.Greys)\n",
    "\n",
    "    y1 = np.linspace(y_lim[0], y_lim[1], 100)\n",
    "    surf = plt.contourf(xm, ym, Z.T, 30, alpha=.8,  cmap=cm.plasma)\n",
    "    \n",
    "    ax3.set_xlim(*x_lim)\n",
    "    ax3.set_ylim(*y_lim)\n",
    "    fig.colorbar(surf, aspect=18, orientation='horizontal')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "def plot_distributions_walks(cbars, drift, noise, gamma):\n",
    "    samples = 1000\n",
    "        \n",
    "    x_lim = [0, 10]\n",
    "    y_lim = [0, 10]\n",
    "\n",
    "    # find gradients of intersection w monotone region -> define affine hedging regions\n",
    "    monotone_point = [4,4]\n",
    "    grads = get_grads_of_intersection(cbars, monotone_point) \n",
    "    annotations = get_hedging_region_annotations(grads, [0,0], x_lim, y_lim)\n",
    "    annotations += get_load_annotations([0,0], load)\n",
    "\n",
    "    start = [8,  8 * grad] # start on intersection\n",
    "    seconds = 30\n",
    "    noise_per_second = 5\n",
    "    \n",
    "    rw, rrw = generate_many_illustrative_paths(samples, start, drift, grad, noise, seconds, noise_per_second)\n",
    "    \n",
    "    mean_rw = np.mean(rw, axis=0)\n",
    "    q1_rw =np.quantile(rw, 0.25, axis=0)\n",
    "    q3_rw = np.quantile(rw, 0.75, axis=0)\n",
    "    \n",
    "    mean_rrw = np.mean(rrw, axis=0)\n",
    "    q1_rrw =np.quantile(rrw, 0.25, axis=0)\n",
    "    q3_rrw = np.quantile(rrw, 0.75, axis=0)\n",
    "\n",
    "    rw_dist = [mean_rw, q1_rw, q3_rw]\n",
    "    rrw_dist = [mean_rrw, q1_rrw, q3_rrw]\n",
    "\n",
    "    dist = [rw_dist, rrw_dist]\n",
    "    plot_distribs(cbars, dist, annotations, rrw, gamma, x_lim=x_lim, y_lim=y_lim)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Hedging \n",
    "\n",
    "Now that we have the transformation matrix $\\workloadmathat$, our problem optimisation can be expressed in workload space:\n",
    "\n",
    "$$\\bar{c}(\\worktime) = \\min_{x}  \\cost^\\T \\state \\text{ s.t. } \\workloadmathat \\state = \\worktime $$\n",
    "\n",
    "where the minimisation over $\\state$ for a fixed $\\worktime$ arises due to the multiple possible solutions in to $ \\workloadmathat \\state = \\worktime$.\n",
    "\n",
    "To solutions to the this equation are piecewise linear, as can be seen by rewriting the equation  \n",
    "\n",
    "\n",
    "The cost is piecewise linear and convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find $w = \\hat{\\Xi}x$\n",
    "2. Find the nearest face of the closest face of monotone regions\n",
    "    1. This looks line a sort of line search method. \n",
    "        * Why two vectors?\n",
    "        * Do we have to try all faces?\n",
    "    2. Find the coordinate direction with the shortest distance to this plane (\"projection\")\n",
    "3. Compute asymptotic covariance of the workload-space random walk\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find $w = \\hat{\\Xi}x$\n",
    "2. Find the nearest face of the closest face of monotone regions\n",
    "    1. This looks line a sort of line search method. \n",
    "        * Why two vectors?\n",
    "        * Do we have to try all faces?\n",
    "    2. Find the coordinate direction with the shortest distance to this plane (\"projection\")\n",
    "3. Compute asymptotic covariance of the workload-space random walk\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Duality and Optimisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "*Aside*\n",
    "\n",
    "Although not a formal proof, this duality can be seen from the Lagrangian formulation of the former:\n",
    "\n",
    "$$T* = \\max_{\\workload, \\bottleneck \\geq \\mathbf{0}} \\min T + \\workload^\\T(   \\flprocessing \\flactivitycumulfree +\\mathbf{q}_1 -\\mathbf{q}_2   ) + \\bottleneck^\\T(  \\constituency \\flactivitycumulfree - \\mathbf{T}) + [\\epsilon_1^\\T(-\\flactivitycumulfree) + \\epsilon_2 (-T)]$$ \n",
    "\n",
    "where $\\workload, \\bottleneck$ are Lagrange multipliers or KKT variables. Note that due to the inequality conditions, $\\bottleneck$ is non-negative. This is very similar to the latter's Lagrangian formulation, with  $\\flactivitycumulfree$ and $T$ as KKT variables:\n",
    "\n",
    "$$ \\min_{\\flactivitycumulfree \\geq \\mathbf{0}, T\\geq 0} \\max\\workload^\\T( \\mathbf{q}_1 -  \\mathbf{q}_2) + \\flactivitycumulfree^\\T(\\flprocessing^\\T \\workload + \\constituency^\\T\\bottleneck) + T(-\\mathbf{1}^\\T\\bottleneck + 1 ) + [\\epsilon_1^\\T \\bottleneck]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Your Network\n",
    "\n",
    "$\\constituency $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Hedging \n",
    "\n",
    "Now that we have the transformation matrix $\\workloadmathat$, our problem optimisation can be expressed in workload space:\n",
    "\n",
    "$$\\bar{c}(\\worktime) = \\min_{x}  \\cost^\\T \\state \\text{ s.t. } \\workloadmathat \\state = \\worktime $$\n",
    "\n",
    "where the minimisation over $\\state$ for a fixed $\\worktime$ arises due to the multiple possible solutions in to $ \\workloadmathat \\state = \\worktime$.\n",
    "\n",
    "To solutions to the this equation are piecewise linear, as can be seen by rewriting the equation  \n",
    "\n",
    "\n",
    "The cost is piecewise linear and convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find $w = \\hat{\\Xi}x$\n",
    "2. Find the nearest face of the closest face of monotone regions\n",
    "    1. This looks line a sort of line search method. \n",
    "        * Why two vectors?\n",
    "        * Do we have to try all faces?\n",
    "    2. Find the coordinate direction with the shortest distance to this plane (\"projection\")\n",
    "3. Compute asymptotic covariance of the workload-space random walk\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_{\\mathbf{z}, T} T  \\\\\n",
    "\\\\\\text{ s.t.} \n",
    "\\\\ \\mathbf{q}_2 = \\mathbf{q}_1 + \\flprocessing \\flactivitycumulfree  \\\\\n",
    "\\constituency \\flactivitycumulfree \\leq \\mathbf{T} \\\\\n",
    "\\flactivitycumulfree \\geq \\mathbf{0} \\\\\n",
    "T \\geq 0 \n",
    "$$\n",
    "\n",
    "to which the equivalent dual is: \n",
    "\n",
    "$$\\max_{\\workload, \\bottleneck} \\workload^\\T( \\mathbf{q}_1 -  \\mathbf{q}_2)\n",
    "\\\\\\text{ s.t.} \n",
    "\\\\   \\flprocessing^\\T \\workload + \\constituency^\\T\\bottleneck \\geq \\mathbf{0}  \n",
    "\\\\- \\mathbf{1}^\\T\\bottleneck + 1 \\geq 0 \n",
    "\\\\ \\bottleneck \\geq \\mathbf{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we let  $\\mathbf{q}_1 = \\flarrival$, $\\mathbf{q}_2 = 0$ - i.e we drain to empty from an initial state of $\\flarrival$ - we get the succinct linear optimisation to find our $\\workload$.\n",
    "\n",
    "$$\\min_{\\workload, \\bottleneck} \\workload^T\\flarrival\\\\\n",
    "\\text{ s.t.} \\\\  - \\flprocessing^T \\workload - \\constituency^T\\bottleneck \\leq \\mathbf{0}  \\\\\n",
    "\\mathbf{1}^T\\bottleneck \\leq 1 \\\\\n",
    " \\bottleneck \\geq \\mathbf{0}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
